{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Export\n",
    "\n",
    "In this notebook we export a series of .txt files containing a variety of different data sets. For our purposes here, we export narrative texts only from two general classes of texts: texts traditionally labeled as \"Early Biblical Hebrew\" and texts considered \"Late Biblical Hebrew\".\n",
    "\n",
    "There are three data sets that we export here:<br>\n",
    "1. [clause consituent (phrase) functions](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/function) per clause in late/early Biblical Hebrew sources\n",
    "2. [phrase constituent](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/pdp) functions (words, also known as \"part of speech\") per clause in late/early Biblical Hebrew sources.\n",
    "3. [clause types](https://etcbc.github.io/text-fabric-data/features/hebrew/etcbc4c/typ) (clause) per clause in late/early Biblical Hebrew sources. \n",
    "\n",
    "The data is accessed using [Text-Fabric](https://github.com/ETCBC/text-fabric), a python package made specially for accessing copora like the ETCBC Hebrew database. \n",
    "\n",
    "## Load Text-Fabric and ETCBC Syntactic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from tf.fabric import Fabric # for Text-Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 2.3.7\n",
      "Api reference : https://github.com/ETCBC/text-fabric/wiki/Api\n",
      "Tutorial      : https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb\n",
      "Data sources  : https://github.com/ETCBC/text-fabric-data\n",
      "Data docs     : https://etcbc.github.io/text-fabric-data\n",
      "Shebanq docs  : https://shebanq.ancient-data.org/text\n",
      "Slack team    : https://shebanq.slack.com/signup\n",
      "Questions? Ask shebanq@ancient-data.org for an invite to Slack\n",
      "108 features found and 0 ignored\n"
     ]
    }
   ],
   "source": [
    "# instantiate Text-Fabric (TF) objects\n",
    "\n",
    "TF = Fabric(modules='hebrew/etcbc4c') # load ETCBC Hebrew database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00s loading features ...\n",
      "   |     0.08s Feature overview: 102 for nodes; 5 for edges; 1 configs; 7 computed\n",
      "  8.64s All features loaded/computed - for details use loadLog()\n"
     ]
    }
   ],
   "source": [
    "# load features for linguistic objects (i.e. clauses, phrases, words) from the database\n",
    "\n",
    "# features loaded in a string, space separated\n",
    "api = TF.load('''\n",
    "              book chapter verse\n",
    "              typ pdp function\n",
    "              domain\n",
    "              ''')\n",
    "\n",
    "# TF classes are globalized for easier use\n",
    "api.makeAvailableIn(globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather, Arrange, and Export Data\n",
    "\n",
    "ETCBC data is stored in graph structure with linguistic objects existing as nodes that have corresponding features. TF uses a node integer to access a dictionary and pull the requested feature with a function: `F.feature.v(node_number)`. There are various other functions used to iterate through the nodes which you can explore more thoroughly in the tutorial [here](https://github.com/codykingham/tfNotebooks/blob/master/timeSpans/Text_Fabric_Tutorial.ipynb) or [here](https://github.com/ETCBC/text-fabric/blob/master/docs/tutorial.ipynb). There are also edge relationships between some nodes (such as clause relations which represent the discourse structure of the text).\n",
    "\n",
    "\n",
    "### Functions for Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_hebrew = {'Genesis', 'Exodus', 'Leviticus', \n",
    "                'Deuteronomy', 'Joshua', 'Judges',\n",
    "                '1_Samuel', '2_Samuel', '1_Kings',\n",
    "                '2_Kings'}\n",
    "\n",
    "late_hebrew = {'Esther', 'Ezra', 'Nehemiah',\n",
    "               '1_Chronicles', '2_Chronicles'}\n",
    "\n",
    "def get_data(feature, obtype):\n",
    "    \n",
    "    '''\n",
    "    Returns dictionary with linguistic date as key and list as value.\n",
    "    List contains space-separated strings of word/phrase level functions.\n",
    "    Requires the feature and ETCBC object type.\n",
    "    '''\n",
    "    \n",
    "    function_data = collections.defaultdict(list)\n",
    "\n",
    "    for book in F.otype.s('book'):\n",
    "\n",
    "        if F.book.v(book) in early_hebrew:\n",
    "            book_tag = 'EBH'\n",
    "        elif F.book.v(book) in late_hebrew:\n",
    "            book_tag = 'LBH'\n",
    "        else: # skip irrelevant books\n",
    "            continue\n",
    "\n",
    "        # get all clauses in the book\n",
    "        book_clauses = L.d(book, otype='clause')\n",
    "\n",
    "        # add phrase data per clause\n",
    "        for clause in book_clauses:\n",
    "\n",
    "            # format data for all phrases in the clause\n",
    "            phrase_functions = [feature.v(obj) for obj in L.d(clause, otype=obtype)]\n",
    "            phrase_funct_str = ' '.join(phrase_functions)\n",
    "\n",
    "            function_data[book_tag].append(phrase_funct_str) # save data\n",
    "            \n",
    "    return(function_data)\n",
    "     \n",
    "    \n",
    "def export_dated_files(data_dict, file_name):\n",
    "    \n",
    "    '''\n",
    "    Exports simple data txt files per dated text.\n",
    "    '''\n",
    "    \n",
    "    for linguistic_date, linguistic_data in data_dict.items():\n",
    "\n",
    "        filename = file_name.format(linguistic_date)\n",
    "\n",
    "        with open(filename, 'w') as outfile:\n",
    "\n",
    "            for phrase in linguistic_data:\n",
    "                outfile.write(phrase+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase Constituents (words and their parts of speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prep subs verb subs prep art subs conj prep art subs'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_function_data = get_data(F.pdp, 'word')\n",
    "                              \n",
    "word_function_data['EBH'][0] # sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export file\n",
    "export_dated_files(word_function_data, 'word_functions_{}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clause Constituents (phrases and their functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Time Pred Subj Objc'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply function\n",
    "phrase_function_data = get_data(F.function, 'phrase')\n",
    "phrase_function_data['EBH'][0] # sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export file\n",
    "export_dated_files(phrase_function_data, 'phrase_functions_{}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clause Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xQtX'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clause_type_data = get_data(F.typ, 'clause_atom')\n",
    "clause_type_data['EBH'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# export file\n",
    "export_dated_files(clause_type_data, 'clause_types_{}.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
